{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOg7mCOZao/JuSivjBzgull",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/junkuna/Machine-Learning-Training/blob/main/Cross_Validation_and_Grid_Search_(Wine)_5_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8lbGi0hst0dy",
        "outputId": "0bde681c-023d-4829-cb02-7af005304ef0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(4157, 3) (1040, 3)\n",
            "The validation sets of traing and test scores are\n",
            "0.9971133028626413\n",
            "0.864423076923077\n",
            "It is overfitting, we will do 5-fold cross validation sets\n",
            "{'fit_time': array([0.05572867, 0.07897329, 0.02334261, 0.02938819, 0.02996111]), 'score_time': array([0.01463747, 0.00222802, 0.00216722, 0.00963736, 0.00209546]), 'test_score': array([0.86923077, 0.84615385, 0.87680462, 0.84889317, 0.83541867])}\n",
            "The final score of fold cross validation is\n",
            "0.855300214703487\n",
            "The mixed five cross validation score is 0.855300214703487\n",
            "0.8574181117533719\n",
            "Training with hiper parameter of score is  0.9615162593804117\n",
            "DecisionTreeClassifier(min_impurity_decrease=0.0001, random_state=42)\n",
            "For each scores of five times training of cross validation sets are\n",
            "{'min_impurity_decrease': 0.0001}\n",
            "[0.86819297 0.86453617 0.86492226 0.86780891 0.86761605]\n",
            "{'min_impurity_decrease': 0.0001}\n",
            "{'max_depth': 14, 'min_impurity_decrease': 0.0004, 'min_samples_split': 12}\n",
            "0.8683865773302731\n",
            "{'max_depth': 46, 'min_impurity_decrease': 0.000517411003148779, 'min_samples_leaf': 5, 'min_samples_split': 2}\n",
            "0.8685788850225808\n",
            "0.8685788850225808\n",
            "0.8584615384615385\n"
          ]
        }
      ],
      "source": [
        "# Prevent not to use test sets in the training process\n",
        "import pandas as pd\n",
        "wine = pd.read_csv('https://raw.githubusercontent.com/rickiepark/hg-mldl/master/wine.csv')\n",
        "\n",
        "data = wine[['alcohol', 'sugar', 'pH']].to_numpy()\n",
        "target = wine['class'].to_numpy()\n",
        "\n",
        "## we will make validation set to prevent using many test sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "train_input, test_input, train_target, test_target = train_test_split(\n",
        "    data, target, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "sub_input, val_input, sub_target, val_target = train_test_split(\n",
        "    train_input, train_target, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "print(sub_input.shape, val_input.shape)\n",
        "\n",
        "## training using validation sets\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "dt=DecisionTreeClassifier(random_state=42)\n",
        "dt.fit(sub_input, sub_target)\n",
        "print('The validation sets of traing and test scores are')\n",
        "print(dt.score(sub_input, sub_target))\n",
        "print(dt.score(val_input, val_target))\n",
        "print('It is overfitting, we will do 5-fold cross validation sets')\n",
        "\n",
        "## produce 5-fold cross validation sets and check score\n",
        "from sklearn.model_selection import cross_validate\n",
        "scores = cross_validate(dt, train_input, train_target)\n",
        "print(scores)\n",
        "\n",
        "##final score of 5-fold cross validation\n",
        "import numpy as np\n",
        "print('The final score of fold cross validation is')\n",
        "print(np.mean(scores['test_score']))\n",
        "\n",
        "## split the folds into five cross validation sets to mix cross validation\n",
        "### cross_validate supports how to split into many folds\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "scores = cross_validate(dt, train_input, train_target, cv = StratifiedKFold())\n",
        "print('The mixed five cross validation score is', np.mean(scores['test_score']))\n",
        "\n",
        "## for ten cross validation sets\n",
        "splitter = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
        "scores = cross_validate(dt, train_input, train_target, cv=splitter)\n",
        "print(np.mean(scores['test_score']))\n",
        "\n",
        "## we will find better model using hiper parameter\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "params = {'min_impurity_decrease': [0.0001, 0.0002, 0.0003, 0.0004, 0.0005]}\n",
        "gs = GridSearchCV(DecisionTreeClassifier(random_state=42), params, n_jobs=-1)\n",
        "\n",
        "gs.fit(train_input, train_target)\n",
        "\n",
        "### after the training parameter, it will automatically train the hole train sets\n",
        "### it is not necessary to make train sets after hiper parameter because of GridSearchCV\n",
        "\n",
        "dt = gs.best_estimator_\n",
        "print('Training with hiper parameter of score is ', dt.score(train_input, train_target))\n",
        "print(gs.best_estimator_)\n",
        "\n",
        "### check the score of five times training of cross validation sets\n",
        "print('For each scores of five times training of cross validation sets are')\n",
        "print(gs.best_params_)\n",
        "print(gs.cv_results_['mean_test_score'])\n",
        "\n",
        "best_index = np.argmax(gs.cv_results_['mean_test_score'])\n",
        "print(gs.cv_results_['params'][best_index])\n",
        "\n",
        "### regulate the impurity, tree depth, and minimize node samples\n",
        "params = {'min_impurity_decrease': np.arange(0.0001, 0.001, 0.0001),\n",
        "          'max_depth': range(5, 20, 1),\n",
        "          'min_samples_split': range(2, 100, 10)\n",
        "          }\n",
        "\n",
        "#### let's try using best parameter with 5 cross validation set\n",
        "gs = GridSearchCV(DecisionTreeClassifier(random_state=42), params, n_jobs=-1)\n",
        "gs.fit(train_input, train_target)\n",
        "print(gs.best_params_)\n",
        "print(np.max(gs.cv_results_['mean_test_score']))\n",
        "\n",
        "### We need more details about the scale of parameter settings through 'Random Search'\n",
        "### It will save more time and large scale for searching the perfact parameter than 'Grid Search'\n",
        "from scipy.stats import uniform, randint\n",
        "\n",
        "params = {'min_impurity_decrease': uniform(0.0001, 0.001),\n",
        "          'max_depth': randint(20, 50),\n",
        "          'min_samples_split': (2, 25),\n",
        "          'min_samples_leaf': randint(1, 25),\n",
        "          }\n",
        "\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "gs = RandomizedSearchCV(DecisionTreeClassifier(random_state=42), params,\n",
        "                        n_iter=100, n_jobs=-1, random_state=42)\n",
        "gs.fit(train_input, train_target)\n",
        "print(gs.best_params_)\n",
        "\n",
        "print(np.max(gs.cv_results_['mean_test_score']))\n",
        "\n",
        "### the perfact model is in train set already in best_esimator_\n",
        "print(np.max(gs.cv_results_['mean_test_score']))\n",
        "dt = gs.best_estimator_\n",
        "print(dt.score(test_input, test_target))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2sRZtAj9wOwc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "6M_4Zw19LSpu"
      }
    }
  ]
}